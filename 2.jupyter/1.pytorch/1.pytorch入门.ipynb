{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###1.tensors使用\n",
    "tensor 类似numpy的ndarray，唯一的区别是Tensor可以在GPU上加速运算。\n",
    "文档地址：https://pytorch.org/docs/torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.6273e+15, 4.5850e-41, 2.8768e+13],\n",
      "        [4.5850e-41, 2.8768e+13, 4.5850e-41],\n",
      "        [1.6357e+15, 4.5850e-41, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 2.5610e-26]])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "\n",
    "#创建空矩阵\n",
    "x = torch.empty(5,3)\n",
    "print(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2891, 0.0370, 0.1284, 0.4624],\n",
      "        [0.4156, 0.2477, 0.5602, 0.2877],\n",
      "        [0.8501, 0.1543, 0.1121, 0.4336],\n",
      "        [0.1453, 0.0318, 0.5344, 0.9588]])\n"
     ]
    }
   ],
   "source": [
    "#随机矩阵\n",
    "x = torch.rand(4,4)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "#long型全0矩阵\n",
    "x = torch.zeros(4,4,dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4.])\n"
     ]
    }
   ],
   "source": [
    "#数组直接构建tensor\n",
    "x = torch.tensor([1.0,2,3,4])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从一个已有的tensor构建一个tensor。这些方法会重用原来tensor的特征，例如，数据类型，除非提供新的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[-0.4087, -0.0469, -1.3417],\n",
      "        [ 0.9152, -0.2320, -0.9257],\n",
      "        [ 0.1312,  0.5948, -1.4433],\n",
      "        [ 0.0054, -0.0315,  0.3638]])\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(4,3,dtype=torch.double) #\n",
    "print(x)\n",
    "\n",
    "x = torch.randn_like(x,dtype=torch.float) #修改类型\n",
    "print(x) #size 跟以前一样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到tensor形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0480,  0.9206, -0.4493],\n",
      "        [ 1.1662,  0.2660, -0.3649],\n",
      "        [ 0.1442,  0.9102, -0.8869],\n",
      "        [ 0.4269,  0.4418,  0.7345]])\n",
      "tensor([[-0.0480,  0.9206, -0.4493],\n",
      "        [ 1.1662,  0.2660, -0.3649],\n",
      "        [ 0.1442,  0.9102, -0.8869],\n",
      "        [ 0.4269,  0.4418,  0.7345]])\n",
      "tensor([[-0.0480,  0.9206, -0.4493],\n",
      "        [ 1.1662,  0.2660, -0.3649],\n",
      "        [ 0.1442,  0.9102, -0.8869],\n",
      "        [ 0.4269,  0.4418,  0.7345]])\n"
     ]
    }
   ],
   "source": [
    "#加法运算, 两种方式\n",
    "y = torch.rand(4,3)\n",
    "\n",
    "print(x+y)\n",
    "\n",
    "print(torch.add(x,y))\n",
    "\n",
    "#加法：把输出作为变量\n",
    "\n",
    "result = torch.empty(4,3)\n",
    "torch.add(x,y,out= result)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in-place加法:y=x+y,结果返回到y上 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0480,  0.9206, -0.4493],\n",
      "        [ 1.1662,  0.2660, -0.3649],\n",
      "        [ 0.1442,  0.9102, -0.8869],\n",
      "        [ 0.4269,  0.4418,  0.7345]])\n"
     ]
    }
   ],
   "source": [
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：\n",
    "任何in-place的运算都会以``_``结尾。 举例来说：``x.copy_(y)``, ``x.t_()``, 会改变 ``x``。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各种类似NumPy的indexing都可以在PyTorch tensor上面使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0469, -1.3417],\n",
      "        [-0.2320, -0.9257]])\n"
     ]
    }
   ],
   "source": [
    "print(x[:2,1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resizing: 如果你希望resize/reshape一个tensor，可以使用torch.view："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)\n",
    "print(x.size(), y.size(),z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你有一个只有一个元素的tensor，使用.item()方法可以把里面的value变成Python数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.3520])\n",
      "1.3519645929336548\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###numpy和tensor之间转换\n",
    "\n",
    "Torch Tensor和NumPy array会共享内存，所以改变其中一项也会改变另一项。\n",
    "\n",
    "把Torch Tensor转变成NumPy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(8)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "b = a.numpy() #转numpy\n",
    "print(b) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改变numpy array里面的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3., 3., 3., 3., 3.])\n",
      "[3. 3. 3. 3. 3. 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(2)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把NumPy ndarray转成Torch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 3. 3. 3. 3.]\n",
      "tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a,2,out = a)\n",
    "print (a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有CPU上的Tensor都支持转成numpy或者从numpy转成Tensor。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###CUDA Tensors\n",
    "使用.to方法，Tensor可以被移动到别的device上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    y = torch.ones_like(x,device=device) #在GPU上创建tensor\n",
    "    \n",
    "    x = x.to(device)  #转到gpu，or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\",torch.double)) #转成cpu，并改变type\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用numpy实现两层神经网络\n",
    "一个全连接ReLU神经网络，一个隐藏层，没有bias。用来从x预测y，使用L2 Loss。\n",
    "\n",
    "这一实现完全使用numpy来计算前向神经网络，loss，和反向传播。\n",
    "\n",
    "numpy ndarray是一个普通的n维array。它不知道任何关于深度学习或者梯度(gradient)的知识，也不知道计算图(computation graph)，只是一种用来计算数学运算的数据结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 38734087.95115929\n",
      "10 1028952.7474450135\n",
      "20 233250.72862875566\n",
      "30 81501.23296350492\n",
      "40 34329.87579143994\n",
      "50 16102.555826770851\n",
      "60 8077.598086757977\n",
      "70 4236.492468074377\n",
      "80 2290.8108589729395\n",
      "90 1266.7427263615245\n",
      "100 712.5830936990685\n",
      "110 406.6159627523841\n",
      "120 234.73764331276638\n",
      "130 136.78470039612458\n",
      "140 80.3338901554973\n",
      "150 47.49719957273632\n",
      "160 28.247692714320845\n",
      "170 16.88528283173057\n",
      "180 10.139165346063715\n",
      "190 6.113354587369562\n",
      "200 3.7002303974501483\n",
      "210 2.247326213442889\n",
      "220 1.3693091213809163\n",
      "230 0.8367703081607656\n",
      "240 0.5127679725172645\n",
      "250 0.31502559137289415\n",
      "260 0.1940281529014829\n",
      "270 0.11979548243856335\n",
      "280 0.07412833819246836\n",
      "290 0.04596685855250905\n",
      "300 0.02856358101070828\n",
      "310 0.017784367764394883\n",
      "320 0.011094608368661708\n",
      "330 0.0069336602301884485\n",
      "340 0.004341153503151124\n",
      "350 0.0027226705583642116\n",
      "360 0.00171046415065889\n",
      "370 0.0010763392937160182\n",
      "380 0.0006784098382189036\n",
      "390 0.00042825060989919997\n",
      "400 0.0002707328674206863\n",
      "410 0.0001714089814507499\n",
      "420 0.00010867201481691581\n",
      "430 6.89963337239582e-05\n",
      "440 4.386297719081903e-05\n",
      "450 2.7919952992696155e-05\n",
      "460 1.7794091949750757e-05\n",
      "470 1.1354596463370516e-05\n",
      "480 7.253619033260029e-06\n",
      "490 4.638934120930828e-06\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N,D_in,H,D_out = 64,1000,100,10\n",
    "\n",
    "#创建随机数据\n",
    "x = np.random.randn(N,D_in)\n",
    "y = np.random.randn(N,D_out)\n",
    "\n",
    "#随机初始化权重 Randomly initialize weights\n",
    "w1 = np.random.randn(D_in,H)\n",
    "w2 = np.random.randn(H,D_out)\n",
    "\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    \n",
    "    #1.前向传播 Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h,0)\n",
    "    y_hat =h_relu.dot(w2)\n",
    "    \n",
    "    #2.计算损失；均方差 compute and print loss\n",
    "    loss = np.square(y_hat - y).sum()\n",
    "    if t %10 ==0:\n",
    "        print(t,loss)\n",
    "    \n",
    "    #3.反向传播\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    \n",
    "    \n",
    "    grad_y_hat =2.0 * (y_hat - y)\n",
    "    \n",
    "    grad_w2 = h_relu.T.dot(grad_y_hat)\n",
    "    grad_h_relu = grad_y_hat.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    #update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###pytorch:tensors 实现两层神经网络\n",
    "\n",
    "我们使用PyTorch tensors来创建前向神经网络，计算损失，以及反向传播。\n",
    "\n",
    "一个PyTorch Tensor很像一个numpy的ndarray。但是它和numpy ndarray最大的区别是，PyTorch Tensor可以在CPU或者GPU上运算。如果想要在GPU上运算，就需要把Tensor换成cuda类型。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 27288992.0\n",
      "10 1751569.625\n",
      "20 250924.796875\n",
      "30 83638.125\n",
      "40 32961.265625\n",
      "50 14367.310546875\n",
      "60 6760.4638671875\n",
      "70 3382.64208984375\n",
      "80 1775.517822265625\n",
      "90 971.0481567382812\n",
      "100 550.3554077148438\n",
      "110 321.5736083984375\n",
      "120 192.9253692626953\n",
      "130 118.420654296875\n",
      "140 74.13906860351562\n",
      "150 47.23534393310547\n",
      "160 30.550188064575195\n",
      "170 20.008148193359375\n",
      "180 13.24726676940918\n",
      "190 8.853684425354004\n",
      "200 5.96598482131958\n",
      "210 4.048614025115967\n",
      "220 2.7644591331481934\n",
      "230 1.8979507684707642\n",
      "240 1.3089643716812134\n",
      "250 0.9065794944763184\n",
      "260 0.6299848556518555\n",
      "270 0.4391378164291382\n",
      "280 0.3068615198135376\n",
      "290 0.2149803340435028\n",
      "300 0.15094204246997833\n",
      "310 0.1061391681432724\n",
      "320 0.07475600391626358\n",
      "330 0.05275623872876167\n",
      "340 0.0373055674135685\n",
      "350 0.026431145146489143\n",
      "360 0.01879294030368328\n",
      "370 0.013401303440332413\n",
      "380 0.009597176685929298\n",
      "390 0.006913910154253244\n",
      "400 0.005027065984904766\n",
      "410 0.0036877659149467945\n",
      "420 0.0027300056535750628\n",
      "430 0.002042273757979274\n",
      "440 0.0015446520410478115\n",
      "450 0.001184122753329575\n",
      "460 0.0009212513105012476\n",
      "470 0.0007284402381628752\n",
      "480 0.0005848477012477815\n",
      "490 0.0004724050231743604\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "#device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "#create random input and output\n",
    "\n",
    "x = torch.randn(N,D_in,device=device,dtype = dtype)\n",
    "y = torch.randn(N,D_out,device = device,dtype = dtype)\n",
    "\n",
    "#randomly initialize weights\n",
    "w1 = torch.randn(D_in,H,device=device,dtype=dtype)\n",
    "w2 = torch.randn(H,D_out,device = device,dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    #forward pass:compute predicted y\n",
    "    h = x.mm(w1) # mm matrix multiplication\n",
    "    h_relu = h.clamp(min=0) #Clamp all elements in input into the range [ min, max ] and return a resulting tensor:\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    \n",
    "    # compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 10 ==0:\n",
    "        print (t, loss)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #backprop to compute gradinets of w1 and w2 with respect to loss\n",
    "    #逆向求梯度\n",
    "    grad_y_pred = 2.0* (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    #update weirghts using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单的autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "#create tensor\n",
    "x = torch.tensor(1.,requires_grad=True)\n",
    "w = torch.tensor(2.,requires_grad=True)\n",
    "b = torch.tensor(3.,requires_grad=True)\n",
    "\n",
    "#build a computational graph\n",
    "y = w*x +b\n",
    "\n",
    "#compute gradients\n",
    "y.backward()\n",
    "\n",
    "#print put the gradients\n",
    "\n",
    "print(x.grad) #对x求偏导，= w\n",
    "print(w.grad) #= x\n",
    "print(b.grad) #= 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-20-abdf51769544>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-abdf51769544>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    PyTorch的一个重要功能就是autograd，也就是说只要定义了forward pass(前向神经网络)，计算了loss之后，PyTorch可以自动求导计算模型所有参数的梯度。\u001b[0m\n\u001b[0m                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###pytorch：tensor和autograd\n",
    "PyTorch的一个重要功能就是autograd，也就是说只要定义了forward pass(前向神经网络)，计算了loss之后，PyTorch可以自动求导计算模型所有参数的梯度。\n",
    "\n",
    "一个PyTorch的Tensor表示计算图中的一个节点。如果x是一个Tensor并且x.requires_grad=True那么x.grad是另一个储存着x当前梯度(相对于一个scalar，常常是loss)的向量。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "N,D_in,H,D_out= 64,1000,100,10\n",
    "\n",
    "x = torch.randn(N,D_in,device=device,dtype=dtype)\n",
    "y = torch.randn(N,D_out,device=device,dtype=dtype)\n",
    "\n",
    "w1 = torch.randn(D_in,H,device=device,dtype=dtype,requires_grad=True)\n",
    "w2 = torch.randn(H,D_out,device=device,dtype=dtype,requires_grad=True)\n",
    "\n",
    "learning_rate=1e-6\n",
    "for t in range(500):\n",
    "    \n",
    "    # 前向传播:通过Tensor预测y；这个和普通的神经网络的前向传播没有任何不同，\n",
    "    # 但是我们不需要保存网络的中间运算结果，因为我们不需要手动计算反向传播。\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    # 通过前向传播计算loss\n",
    "    # loss是一个形状为(1，)的Tensor\n",
    "    # loss.item()可以给我们返回一个loss的scalar\n",
    "    loss = (y_pred -y).pow(2).sum()\n",
    "    if t % 10 ==0:\n",
    "        print(t,loss.item())\n",
    "    \n",
    "    # PyTorch给我们提供了autograd的方法做反向传播。如果一个Tensor的requires_grad=True，\n",
    "    # backward会自动计算loss相对于每个Tensor的gradient。在backward之后，\n",
    "    # w1.grad和w2.grad会包含两个loss相对于两个Tensor的gradient信息。\n",
    "    loss.backward()\n",
    "    \n",
    "    # 我们可以手动做gradient descent(后面我们会介绍自动的方法)。\n",
    "    # 用torch.no_grad()包含以下statements，因为w1和w2都是requires_grad=True，\n",
    "    # 但是在更新weights之后我们并不需要再做autograd。\n",
    "    # 另一种方法是在weight.data和weight.grad.data上做操作，这样就不会对grad产生影响。\n",
    "    # tensor.data会我们一个tensor，这个tensor和原来的tensor指向相同的内存空间，\n",
    "    # 但是不会记录计算图的历史。\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###pytorch:nn nerve network\n",
    "这次我们使用PyTorch中nn这个库来构建网络。 用PyTorch autograd来构建计算图和计算gradients， 然后PyTorch会帮我们自动计算gradient。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. Each Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in,H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H,D_out),\n",
    ")\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    \n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    \n",
    "    y_pred = model(x)\n",
    "    \n",
    "    \n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t %10 ==0:\n",
    "        print (t,loss.item())\n",
    "    \n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    #计算梯度\n",
    "    loss.backward()\n",
    "    \n",
    "    \n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###pytorch:optim\n",
    "提供了不同优化算法\n",
    "这一次我们不再手动更新模型的weights,而是使用optim这个包来帮助我们更新参数。 optim这个package提供了各种不同的模型优化方法，包括SGD+momentum, RMSProp, Adam等等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 626.4588012695312\n",
      "10 482.22283935546875\n",
      "20 376.8345642089844\n",
      "30 295.9488525390625\n",
      "40 232.41639709472656\n",
      "50 181.44467163085938\n",
      "60 139.82139587402344\n",
      "70 106.07933807373047\n",
      "80 78.97282409667969\n",
      "90 57.4207763671875\n",
      "100 40.77534866333008\n",
      "110 28.255468368530273\n",
      "120 18.949909210205078\n",
      "130 12.295174598693848\n",
      "140 7.732590198516846\n",
      "150 4.720807075500488\n",
      "160 2.807846784591675\n",
      "170 1.6349819898605347\n",
      "180 0.9357205629348755\n",
      "190 0.5270295739173889\n",
      "200 0.2925480604171753\n",
      "210 0.16020400822162628\n",
      "220 0.08636952191591263\n",
      "230 0.045787934213876724\n",
      "240 0.023870572447776794\n",
      "250 0.012318517081439495\n",
      "260 0.006234304513782263\n",
      "270 0.003093733685091138\n",
      "280 0.0015040909638628364\n",
      "290 0.0007152080652303994\n",
      "300 0.00033212205744348466\n",
      "310 0.0001504787360318005\n",
      "320 6.648754788329825e-05\n",
      "330 2.8635076887439936e-05\n",
      "340 1.2018940651614685e-05\n",
      "350 4.9112395572592504e-06\n",
      "360 1.9538356355042197e-06\n",
      "370 7.556902232863649e-07\n",
      "380 2.8430241627575015e-07\n",
      "390 1.0387239512965607e-07\n",
      "400 3.672523263276162e-08\n",
      "410 1.2594008502730958e-08\n",
      "420 4.215138460494927e-09\n",
      "430 1.4291974270719265e-09\n",
      "440 5.184807583269446e-10\n",
      "450 2.2469558158544345e-10\n",
      "460 1.242728003836291e-10\n",
      "470 8.65600854882409e-11\n",
      "480 6.566391180795605e-11\n",
      "490 4.991200855397615e-11\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algoriths. The first argument to the Adam constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer =torch.optim.Adam(model.parameters(),lr =learning_rate)\n",
    "\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    loss = loss_fn(y_pred,y)\n",
    "    if t % 10 ==0:\n",
    "        print(t,loss.item())\n",
    "    \n",
    "    \n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    \n",
    "    #参数清零\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    \n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    #求梯度\n",
    "    loss.backward()\n",
    "    \n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    #更新参数 \n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###pytorch:自定义nn Modules\n",
    "\n",
    "我们可以定义一个模型，这个模型继承自nn.Module类。如果需要定义一个比Sequential模型更加复杂的模型，就需要定义nn.Module模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 638.4453735351562\n",
      "10 331.0203857421875\n",
      "20 193.6580047607422\n",
      "30 110.20072174072266\n",
      "40 61.1053352355957\n",
      "50 33.92122268676758\n",
      "60 19.116411209106445\n",
      "70 10.992533683776855\n",
      "80 6.4693827629089355\n",
      "90 3.8941595554351807\n",
      "100 2.3940045833587646\n",
      "110 1.5003877878189087\n",
      "120 0.9564663767814636\n",
      "130 0.6193969249725342\n",
      "140 0.4075735807418823\n",
      "150 0.2725450098514557\n",
      "160 0.18477416038513184\n",
      "170 0.12694892287254333\n",
      "180 0.088416188955307\n",
      "190 0.06234872713685036\n",
      "200 0.04453711956739426\n",
      "210 0.032178498804569244\n",
      "220 0.023498348891735077\n",
      "230 0.01732627861201763\n",
      "240 0.012887961231172085\n",
      "250 0.009663806296885014\n",
      "260 0.0072968280874192715\n",
      "270 0.005544430110603571\n",
      "280 0.004235605709254742\n",
      "290 0.003251543967053294\n",
      "300 0.002506745047867298\n",
      "310 0.0019400939345359802\n",
      "320 0.0015064654871821404\n",
      "330 0.0011733082355931401\n",
      "340 0.000916208082344383\n",
      "350 0.0007171262404881418\n",
      "360 0.0005624534678645432\n",
      "370 0.00044195461669005454\n",
      "380 0.0003478627768345177\n",
      "390 0.0002742106153164059\n",
      "400 0.00021644846128765494\n",
      "410 0.0001710608194116503\n",
      "420 0.0001353403931716457\n",
      "430 0.0001071912920451723\n",
      "440 8.497431554133072e-05\n",
      "450 6.741323886672035e-05\n",
      "460 5.352972220862284e-05\n",
      "470 4.25333455496002e-05\n",
      "480 3.3816158975241706e-05\n",
      "490 2.690396286197938e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#define class  extended nn.Module\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self,D_in,H,D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet,self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in,H)\n",
    "        self.linear2 = torch.nn.Linear(H,D_out)\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min = 0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "\n",
    "#Construct our model by instantiating the class defined above\n",
    "model = TwoLayerNet(D_in,H,D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction = 'sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr =1e-4)\n",
    "for t in range(500):\n",
    "    \n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred,y)\n",
    "    if t %10 ==0:\n",
    "        print(t,loss.item())\n",
    "    \n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###FizzBuzz\n",
    "FizzBuzz是一个简单的小游戏。游戏规则如下：从1开始往上数数，当遇到3的倍数的时候，说fizz，当遇到5的倍数，说buzz，当遇到15的倍数，就说fizzbuzz，其他情况下则正常数数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1).写一个简单的小程序来决定要返回正常数值还是fizz, buzz 或者 fizzbuzz。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "fizz\n",
      "buzz\n",
      "fizzbuzz\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode the desired outputs: [number, \"fizz\", \"buzz\", \"fizzbuzz\"]\n",
    "def fizz_buzz_encode(i):\n",
    "    if   i % 15 == 0: return 3\n",
    "    elif i % 5 == 0: return 2\n",
    "    elif i % 3 == 0:return 1\n",
    "    else: return 0\n",
    "\n",
    "def fizz_buzz_decode(i,prediction):\n",
    "    return [str(i),'fizz','buzz','fizzbuzz'][prediction]\n",
    "\n",
    "print(fizz_buzz_decode(1,fizz_buzz_encode(1)))\n",
    "print(fizz_buzz_decode(2,fizz_buzz_encode(2)))\n",
    "print(fizz_buzz_decode(3,fizz_buzz_encode(3)))\n",
    "print(fizz_buzz_decode(5,fizz_buzz_encode(5)))\n",
    "print(fizz_buzz_decode(15,fizz_buzz_encode(15)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.首先定义模型的输入与输出(训练数据)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 1.1594605445861816\n",
      "Epoch: 99 Loss: 1.1266257762908936\n",
      "Epoch: 198 Loss: 1.0895293951034546\n",
      "Epoch: 297 Loss: 0.9905086159706116\n",
      "Epoch: 396 Loss: 0.8403555154800415\n",
      "Epoch: 495 Loss: 0.709020733833313\n",
      "Epoch: 594 Loss: 0.8986888527870178\n",
      "Epoch: 693 Loss: 0.7472803592681885\n",
      "Epoch: 792 Loss: 0.2779087424278259\n",
      "Epoch: 891 Loss: 0.21065081655979156\n",
      "Epoch: 990 Loss: 0.15177930891513824\n",
      "Epoch: 1089 Loss: 0.12456075102090836\n",
      "Epoch: 1188 Loss: 0.1050400510430336\n",
      "Epoch: 1287 Loss: 0.0901724323630333\n",
      "Epoch: 1386 Loss: 0.07855375856161118\n",
      "Epoch: 1485 Loss: 0.06922419369220734\n",
      "Epoch: 1584 Loss: 0.06179199367761612\n",
      "Epoch: 1683 Loss: 0.05551811680197716\n",
      "Epoch: 1782 Loss: 0.050394997000694275\n",
      "Epoch: 1881 Loss: 0.04600830376148224\n",
      "Epoch: 1980 Loss: 0.042291060090065\n",
      "Epoch: 2079 Loss: 0.03907600790262222\n",
      "Epoch: 2178 Loss: 0.03617529198527336\n",
      "Epoch: 2277 Loss: 0.03338903188705444\n",
      "Epoch: 2376 Loss: 0.031031055375933647\n",
      "Epoch: 2475 Loss: 0.02892322465777397\n",
      "Epoch: 2574 Loss: 0.026990056037902832\n",
      "Epoch: 2673 Loss: 0.025202559307217598\n",
      "Epoch: 2772 Loss: 0.023482372984290123\n",
      "Epoch: 2871 Loss: 0.021916154772043228\n",
      "Epoch: 2970 Loss: 0.02051696553826332\n",
      "Epoch: 3069 Loss: 0.019188042730093002\n",
      "Epoch: 3168 Loss: 0.01801450550556183\n",
      "Epoch: 3267 Loss: 0.017002107575535774\n",
      "Epoch: 3366 Loss: 0.016040192916989326\n",
      "Epoch: 3465 Loss: 0.015210790559649467\n",
      "Epoch: 3564 Loss: 0.014477537013590336\n",
      "Epoch: 3663 Loss: 0.013781631365418434\n",
      "Epoch: 3762 Loss: 0.013155137188732624\n",
      "Epoch: 3861 Loss: 0.012589259073138237\n",
      "Epoch: 3960 Loss: 0.012040572240948677\n",
      "Epoch: 4059 Loss: 0.01154858898371458\n",
      "Epoch: 4158 Loss: 0.011070995591580868\n",
      "Epoch: 4257 Loss: 0.01064454484730959\n",
      "Epoch: 4356 Loss: 0.010234320536255836\n",
      "Epoch: 4455 Loss: 0.009846816770732403\n",
      "Epoch: 4554 Loss: 0.009495718404650688\n",
      "Epoch: 4653 Loss: 0.009157709777355194\n",
      "Epoch: 4752 Loss: 0.0088501563295722\n",
      "Epoch: 4851 Loss: 0.008550719358026981\n",
      "Epoch: 4950 Loss: 0.008277125656604767\n",
      "Epoch: 5049 Loss: 0.008013942278921604\n",
      "Epoch: 5148 Loss: 0.007765761110931635\n",
      "Epoch: 5247 Loss: 0.00753496028482914\n",
      "Epoch: 5346 Loss: 0.007316839415580034\n",
      "Epoch: 5445 Loss: 0.007106248755007982\n",
      "Epoch: 5544 Loss: 0.006909285672008991\n",
      "Epoch: 5643 Loss: 0.006719087716192007\n",
      "Epoch: 5742 Loss: 0.006534823682159185\n",
      "Epoch: 5841 Loss: 0.0063618808053433895\n",
      "Epoch: 5940 Loss: 0.006190035957843065\n",
      "Epoch: 6039 Loss: 0.006029170472174883\n",
      "Epoch: 6138 Loss: 0.005868184845894575\n",
      "Epoch: 6237 Loss: 0.00571811618283391\n",
      "Epoch: 6336 Loss: 0.005574993323534727\n",
      "Epoch: 6435 Loss: 0.005435856990516186\n",
      "Epoch: 6534 Loss: 0.005307224113494158\n",
      "Epoch: 6633 Loss: 0.005183697212487459\n",
      "Epoch: 6732 Loss: 0.005063377320766449\n",
      "Epoch: 6831 Loss: 0.004951821640133858\n",
      "Epoch: 6930 Loss: 0.004841746762394905\n",
      "Epoch: 7029 Loss: 0.004736191127449274\n",
      "Epoch: 7128 Loss: 0.004635557532310486\n",
      "Epoch: 7227 Loss: 0.004540718626230955\n",
      "Epoch: 7326 Loss: 0.004444910679012537\n",
      "Epoch: 7425 Loss: 0.0043577766045928\n",
      "Epoch: 7524 Loss: 0.004270981997251511\n",
      "Epoch: 7623 Loss: 0.004185956437140703\n",
      "Epoch: 7722 Loss: 0.004097835626453161\n",
      "Epoch: 7821 Loss: 0.004017614293843508\n",
      "Epoch: 7920 Loss: 0.003939152229577303\n",
      "Epoch: 8019 Loss: 0.0038656224496662617\n",
      "Epoch: 8118 Loss: 0.0037924812640994787\n",
      "Epoch: 8217 Loss: 0.0037244828417897224\n",
      "Epoch: 8316 Loss: 0.003658122615888715\n",
      "Epoch: 8415 Loss: 0.0035929179284721613\n",
      "Epoch: 8514 Loss: 0.0035296103451400995\n",
      "Epoch: 8613 Loss: 0.003469318151473999\n",
      "Epoch: 8712 Loss: 0.0034110206179320812\n",
      "Epoch: 8811 Loss: 0.003353740321472287\n",
      "Epoch: 8910 Loss: 0.0032991119660437107\n",
      "Epoch: 9009 Loss: 0.0032463192474097013\n",
      "Epoch: 9108 Loss: 0.003195566823706031\n",
      "Epoch: 9207 Loss: 0.003145339200273156\n",
      "Epoch: 9306 Loss: 0.003097166772931814\n",
      "Epoch: 9405 Loss: 0.0030497554689645767\n",
      "Epoch: 9504 Loss: 0.0030037378892302513\n",
      "Epoch: 9603 Loss: 0.002960100769996643\n",
      "Epoch: 9702 Loss: 0.0029161828570067883\n",
      "Epoch: 9801 Loss: 0.002874829340726137\n",
      "Epoch: 9900 Loss: 0.002833947539329529\n",
      "Epoch: 9999 Loss: 0.0027938131242990494\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "NUM_DIGITS=10\n",
    "\n",
    "#represent each input by an array of its binary digits.\n",
    "#把数字转成二进制，一个数字不好训练，转成二进制可以更好的训练\n",
    "def binary_encode(i,num_digits):\n",
    "    return np.array([i >> d & 1 for d in range(num_digits)])\n",
    "\n",
    "trX = torch.Tensor([binary_encode(i,NUM_DIGITS) for i in range(101,2 ** NUM_DIGITS)])\n",
    "trY = torch.LongTensor([fizz_buzz_encode(i) for i in range(101,2 ** NUM_DIGITS)])\n",
    "\n",
    "\n",
    "#2.PyTorch定义模型\n",
    "NUM_HIDDEN = 100\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(NUM_DIGITS,NUM_HIDDEN),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(NUM_HIDDEN,4)\n",
    ")\n",
    "# 是否可以使用cuda\n",
    "if torch.cuda.is_available():\n",
    "    mode = model.cuda()\n",
    "\n",
    "\n",
    "#为了让我们的模型学会FizzBuzz这个游戏，我们需要定义一个损失函数，和一个优化算法。\n",
    "#这个优化算法会不断优化（降低）损失函数，使得模型的在该任务上取得尽可能低的损失值。\n",
    "#损失值低往往表示我们的模型表现好，损失值高表示我们的模型表现差。\n",
    "#由于FizzBuzz游戏本质上是一个分类问题，我们选用Cross Entropy Loss函数。\n",
    "#Cross Entropy Loss：拟合多种分布的相似度有多高。\n",
    "#优化函数我们选用Stochastic Gradient Descent。\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "#定义optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = 0.1)\n",
    "\n",
    "#3.以下是模型的训练代码\n",
    "\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "for epoch in range(10000):\n",
    "    for start in range(0,len(trX),BATCH_SIZE):\n",
    "        end = start + BATCH_SIZE\n",
    "        batchX = trX[start:end]\n",
    "        batchY = trY[start:end]\n",
    "        y_pred = model(batchX)\n",
    "        \n",
    "        loss = loss_fn(y_pred,batchY)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # find loss on training data\n",
    "    loss = loss_fn(model(trX),trY).item()\n",
    "    if epoch % 99 ==0: \n",
    "        print('Epoch:',epoch,'Loss:',loss)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.我们用训练好的模型尝试在1到100这些数字上玩FizzBuzz游戏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2038e+01, -8.5395e+00, -4.8167e+00,  5.9974e-01],\n",
      "        [ 6.1152e+00, -2.0043e+00, -4.0197e+00,  1.9991e-01],\n",
      "        [-2.0113e+00,  6.7697e+00, -3.5793e+00, -1.8476e+00],\n",
      "        [ 1.0731e+01, -3.0169e-01,  7.3719e-01, -1.1266e+01],\n",
      "        [-1.1435e+00, -5.6088e+00,  1.0572e+01, -3.7779e+00],\n",
      "        [-5.4797e+00,  7.7131e+00,  1.4950e+00, -4.1646e+00],\n",
      "        [ 6.8943e+00, -7.9624e+00,  1.8340e+00, -1.2174e+00],\n",
      "        [ 7.5730e+00, -2.6080e+00, -1.8484e+00, -3.2641e+00],\n",
      "        [ 3.5401e+00,  6.1937e+00, -5.8304e+00, -5.1532e+00],\n",
      "        [ 4.6590e-01, -5.8496e+00,  8.2583e+00, -2.1719e+00],\n",
      "        [ 8.5198e+00, -5.0074e+00,  6.9479e-01, -4.3416e+00],\n",
      "        [ 4.1876e-01,  1.0214e+01, -5.7481e+00, -5.3277e+00],\n",
      "        [ 1.1851e+01, -2.2556e+00, -5.8587e+00, -4.3487e+00],\n",
      "        [ 1.1219e+01, -5.4680e+00, -5.0573e+00, -2.3178e-01],\n",
      "        [-3.3820e+00, -1.3165e+00, -4.3443e+00,  9.1054e+00],\n",
      "        [ 5.5784e+00, -8.0226e+00, -2.7190e-01,  2.5457e+00],\n",
      "        [ 1.0173e+01, -6.0593e+00, -2.5795e+00, -2.1863e+00],\n",
      "        [-4.9146e+00,  9.3681e+00, -2.1998e+00, -2.5778e+00],\n",
      "        [ 1.0756e+01, -1.1627e+00, -4.8377e-01, -9.7972e+00],\n",
      "        [-8.3120e-01, -2.6019e+00,  1.1320e+01, -8.0055e+00],\n",
      "        [ 2.8268e+00,  2.6226e+00, -1.7948e-01, -5.4582e+00],\n",
      "        [ 9.4911e+00, -3.8200e+00, -8.4200e-01, -5.4736e+00],\n",
      "        [ 8.5230e+00, -3.8878e+00,  2.3743e-01, -5.3601e+00],\n",
      "        [-2.7957e+00,  4.5275e+00, -9.0734e-01, -1.5487e+00],\n",
      "        [ 3.9813e+00, -3.6481e+00,  5.4816e+00, -7.1049e+00],\n",
      "        [ 7.6456e+00, -7.3415e+00,  2.8070e+00, -2.7333e+00],\n",
      "        [ 1.1597e+00,  4.1662e+00, -2.3143e+00, -3.6960e+00],\n",
      "        [ 7.6628e+00, -2.7759e+00, -2.7356e+00, -2.7022e+00],\n",
      "        [ 1.3169e+01, -3.2803e+00, -6.2781e+00, -4.2129e+00],\n",
      "        [-3.6523e+00, -8.6432e-03, -5.8499e+00,  9.3686e+00],\n",
      "        [ 9.9339e+00, -5.3316e+00, -3.2899e+00, -1.4503e+00],\n",
      "        [ 6.1990e+00, -2.2378e+00, -3.8727e+00, -3.0515e-01],\n",
      "        [ 3.1286e+00,  4.5166e+00, -4.4779e+00, -4.1401e+00],\n",
      "        [ 8.5397e+00,  1.9838e+00, -5.3147e+00, -5.1738e+00],\n",
      "        [ 4.9035e+00, -4.7817e+00,  4.8332e+00, -5.5741e+00],\n",
      "        [-4.2380e+00,  1.2690e+01, -1.9843e+00, -6.9628e+00],\n",
      "        [ 8.1356e+00, -5.4706e+00,  8.0751e-01, -4.1143e+00],\n",
      "        [ 4.2645e+00,  1.2477e+00, -1.8846e+00, -4.1331e+00],\n",
      "        [-3.3594e+00,  6.3503e+00, -2.1078e+00, -1.8749e+00],\n",
      "        [ 1.4842e+00, -1.3748e+00,  3.9372e+00, -3.6742e+00],\n",
      "        [ 1.2049e+01, -2.7598e+00, -4.0483e+00, -5.8149e+00],\n",
      "        [-2.4906e-01,  5.7365e+00, -6.4790e+00,  2.2256e+00],\n",
      "        [ 6.8760e+00, -3.7042e+00, -4.0027e+00,  1.1048e+00],\n",
      "        [ 9.9320e+00,  3.1903e-01, -7.7519e+00, -2.3490e+00],\n",
      "        [ 3.7264e+00, -3.1094e+00, -6.7847e+00,  5.9461e+00],\n",
      "        [ 9.4977e+00, -3.6687e+00, -4.2957e+00, -6.6963e-01],\n",
      "        [ 5.5045e+00, -4.0210e+00,  1.0407e+00, -2.0655e+00],\n",
      "        [-2.2290e+00,  4.5364e+00,  9.2020e-01, -3.5599e+00],\n",
      "        [ 1.3083e+01, -2.4941e+00, -1.7296e+00, -9.7423e+00],\n",
      "        [-2.8076e+00,  2.0824e+00,  5.5060e+00, -4.9916e+00],\n",
      "        [-7.7123e-01,  6.9027e+00, -2.4382e+00, -4.6327e+00],\n",
      "        [ 8.3611e+00, -3.2159e+00,  1.6303e+00, -7.2947e+00],\n",
      "        [ 9.4471e+00, -3.2484e+00,  2.9580e-01, -6.9482e+00],\n",
      "        [-4.5406e+00,  1.4407e+01, -5.4316e+00, -5.1060e+00],\n",
      "        [ 8.2150e-01, -8.8737e-01,  8.2078e+00, -8.6390e+00],\n",
      "        [ 7.5838e+00, -5.9786e+00,  1.7671e+00, -3.3237e+00],\n",
      "        [ 2.2731e+00,  4.8124e+00, -4.5050e+00, -3.4376e+00],\n",
      "        [ 8.1062e+00, -7.7021e+00, -9.3251e-01,  1.6038e+00],\n",
      "        [ 1.1049e+01, -4.9144e+00, -5.3865e+00, -5.9844e-01],\n",
      "        [ 1.9245e+00, -3.7588e+00, -4.0628e+00,  5.7697e+00],\n",
      "        [ 1.3355e+01, -6.6557e+00, -4.1954e+00, -2.9618e+00],\n",
      "        [ 5.7737e+00, -2.7430e+00, -4.6469e-02, -2.2570e+00],\n",
      "        [-3.5525e+00,  6.1564e+00,  1.3841e+00, -3.8068e+00],\n",
      "        [ 1.1021e+01, -1.9499e+00, -3.8276e-01, -8.7157e+00],\n",
      "        [-2.2705e+00, -5.6063e+00,  9.3566e+00, -2.3719e+00],\n",
      "        [-3.8056e+00,  6.3105e+00, -1.5490e-01, -2.7291e+00],\n",
      "        [ 8.8533e+00, -9.5293e+00,  1.4783e+00, -2.1071e+00],\n",
      "        [ 6.4577e+00,  5.0069e+00,  4.2684e+00, -1.6579e+01],\n",
      "        [ 2.4211e+00,  6.4330e+00,  1.3651e-01, -1.0026e+01],\n",
      "        [-3.8865e+00, -2.0329e+00,  1.3171e+01, -8.3964e+00],\n",
      "        [ 6.1752e+00, -2.0129e+00,  7.4962e-01, -6.5856e+00],\n",
      "        [ 2.9289e-01,  9.5860e+00, -6.8884e+00, -3.5504e+00],\n",
      "        [ 8.3600e+00, -3.7369e+00, -3.7435e+00, -2.0306e+00],\n",
      "        [ 8.3263e+00, -5.0469e+00, -4.1836e+00,  1.3058e+00],\n",
      "        [-8.4595e+00, -1.5706e+00, -5.4102e-01,  9.7616e+00],\n",
      "        [ 1.0647e+01,  1.2485e+00, -2.3124e+00, -1.0546e+01],\n",
      "        [ 8.4829e+00, -1.4331e+00, -1.9979e-02, -8.2673e+00],\n",
      "        [-9.0704e-01,  9.8488e+00, -2.5312e+00, -6.9187e+00],\n",
      "        [ 6.9393e+00, -2.0726e+00,  1.2287e+00, -7.2346e+00],\n",
      "        [-4.2800e-01, -2.2482e-01,  7.1650e+00, -7.2531e+00],\n",
      "        [-3.5051e+00,  4.9433e+00,  2.2982e-01, -3.0982e+00],\n",
      "        [ 9.9715e+00, -3.5299e+00, -1.6610e+00, -5.8280e+00],\n",
      "        [ 4.4019e+00, -2.5011e+00,  1.1336e+00, -4.8951e+00],\n",
      "        [ 3.4370e+00,  8.0868e+00,  2.9650e+00, -1.5564e+01],\n",
      "        [-4.8840e+00, -8.7241e-01,  1.2798e+01, -8.0546e+00],\n",
      "        [ 5.7956e+00,  1.2116e+00,  7.7992e-01, -9.5011e+00],\n",
      "        [ 8.6811e-02,  2.7538e+00,  4.8970e-01, -5.1833e+00],\n",
      "        [ 5.5029e+00,  3.2044e-01, -4.1070e+00, -2.7763e+00],\n",
      "        [ 8.6827e+00, -2.2758e+00, -5.3554e+00, -2.5126e+00],\n",
      "        [-2.0850e+00, -9.4062e-01, -4.9282e+00,  7.2934e+00],\n",
      "        [ 7.3156e+00, -5.5637e+00, -1.7652e+00, -1.1673e+00],\n",
      "        [ 8.8526e+00,  1.4055e-02, -8.1972e-01, -9.1989e+00],\n",
      "        [ 5.2295e+00,  3.6921e+00,  5.3830e-01, -1.0550e+01],\n",
      "        [ 6.3716e+00, -3.8826e-01, -1.0572e+00, -6.1085e+00],\n",
      "        [-4.8242e-01, -6.3783e+00,  1.0130e+01, -4.5122e+00],\n",
      "        [ 7.8067e-01,  6.7227e+00, -2.4012e+00, -6.0550e+00],\n",
      "        [ 9.0089e+00, -5.1954e+00, -1.8323e+00, -3.7479e+00],\n",
      "        [ 6.8340e+00, -1.9867e+00, -2.9468e+00, -2.8542e+00],\n",
      "        [ 1.0044e+00,  2.0524e+00, -4.9431e+00, -1.3473e-01],\n",
      "        [ 2.3362e+00, -1.9319e+00,  1.0975e+01, -1.2552e+01]])\n",
      "['1', '2', 'fizz', '4', 'buzz', 'fizz', '7', '8', 'fizz', 'buzz', '11', 'fizz', '13', '14', 'fizzbuzz', '16', '17', 'fizz', '19', 'buzz', '21', '22', '23', 'fizz', 'buzz', '26', 'fizz', '28', '29', 'fizzbuzz', '31', '32', 'fizz', '34', '35', 'fizz', '37', '38', 'fizz', 'buzz', '41', 'fizz', '43', '44', 'fizzbuzz', '46', '47', 'fizz', '49', 'buzz', 'fizz', '52', '53', 'fizz', 'buzz', '56', 'fizz', '58', '59', 'fizzbuzz', '61', '62', 'fizz', '64', 'buzz', 'fizz', '67', '68', 'fizz', 'buzz', '71', 'fizz', '73', '74', 'fizzbuzz', '76', '77', 'fizz', '79', 'buzz', 'fizz', '82', '83', 'fizz', 'buzz', '86', 'fizz', '88', '89', 'fizzbuzz', '91', '92', '93', '94', 'buzz', 'fizz', '97', '98', 'fizz', 'buzz']\n"
     ]
    }
   ],
   "source": [
    "testX = torch.Tensor([binary_encode(i,NUM_DIGITS) for i in range(1,101)])\n",
    "with torch.no_grad(): #在预测的时候不需要gradient，所以用no_grad.\n",
    "    testY = model(testX) #prediction\n",
    "\n",
    "print(testY)\n",
    "predictions = zip(range(1,101), list(testY.max(1)[1].data.tolist())) #testY.max 找出概率最大的那个分类\n",
    "\n",
    "print([fizz_buzz_decode(i,x ) for (i,x) in predictions])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#计算准确率\n",
    "print(np.sum(testY.max(1)[1].numpy() == np.array([fizz_buzz_encode(i) for i in range(1,101)])))\n",
    "\n",
    "testY.max(1)[1].numpy() == np.array([fizz_buzz_encode(i) for i in range(1,101)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
